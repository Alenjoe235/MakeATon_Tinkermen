{"cells":[{"cell_type":"code","execution_count":7,"id":"996bca8a","metadata":{"execution":{"iopub.execute_input":"2024-10-20T02:30:17.356044Z","iopub.status.busy":"2024-10-20T02:30:17.355382Z","iopub.status.idle":"2024-10-20T02:30:26.224256Z","shell.execute_reply":"2024-10-20T02:30:26.223293Z","shell.execute_reply.started":"2024-10-20T02:30:17.356008Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING: Ignoring invalid distribution ~ip (C:\\Python312\\Lib\\site-packages)\n","WARNING: Ignoring invalid distribution ~ip (C:\\Python312\\Lib\\site-packages)\n","WARNING: Ignoring invalid distribution ~ip (C:\\Python312\\Lib\\site-packages)\n","WARNING: Ignoring invalid distribution ~ip (C:\\Python312\\Lib\\site-packages)\n"]},{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pandas in c:\\python312\\lib\\site-packages (2.2.3)\n","Requirement already satisfied: folium in c:\\python312\\lib\\site-packages (0.17.0)\n","Requirement already satisfied: matplotlib in c:\\python312\\lib\\site-packages (3.9.2)\n","Requirement already satisfied: seaborn in c:\\python312\\lib\\site-packages (0.13.2)\n","Requirement already satisfied: numpy in c:\\python312\\lib\\site-packages (2.0.1)\n","Requirement already satisfied: requests in c:\\users\\shravan\\appdata\\roaming\\python\\python312\\site-packages (2.32.3)\n","Requirement already satisfied: google-api-core in c:\\python312\\lib\\site-packages (2.21.0)\n","Requirement already satisfied: google-generativeai in c:\\python312\\lib\\site-packages (0.8.3)\n","Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\shravan\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in c:\\python312\\lib\\site-packages (from pandas) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in c:\\python312\\lib\\site-packages (from pandas) (2024.2)\n","Requirement already satisfied: branca>=0.6.0 in c:\\python312\\lib\\site-packages (from folium) (0.8.0)\n","Requirement already satisfied: jinja2>=2.9 in c:\\python312\\lib\\site-packages (from folium) (3.1.4)\n","Requirement already satisfied: xyzservices in c:\\python312\\lib\\site-packages (from folium) (2024.9.0)\n","Requirement already satisfied: contourpy>=1.0.1 in c:\\python312\\lib\\site-packages (from matplotlib) (1.3.0)\n","Requirement already satisfied: cycler>=0.10 in c:\\python312\\lib\\site-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in c:\\python312\\lib\\site-packages (from matplotlib) (4.54.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in c:\\python312\\lib\\site-packages (from matplotlib) (1.4.7)\n","Requirement already satisfied: packaging>=20.0 in c:\\users\\shravan\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (24.1)\n","Requirement already satisfied: pillow>=8 in c:\\python312\\lib\\site-packages (from matplotlib) (11.0.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in c:\\python312\\lib\\site-packages (from matplotlib) (3.2.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shravan\\appdata\\roaming\\python\\python312\\site-packages (from requests) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shravan\\appdata\\roaming\\python\\python312\\site-packages (from requests) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python312\\lib\\site-packages (from requests) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shravan\\appdata\\roaming\\python\\python312\\site-packages (from requests) (2024.8.30)\n","Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in c:\\python312\\lib\\site-packages (from google-api-core) (1.65.0)\n","Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in c:\\python312\\lib\\site-packages (from google-api-core) (5.28.2)\n","Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\python312\\lib\\site-packages (from google-api-core) (1.24.0)\n","Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in c:\\python312\\lib\\site-packages (from google-api-core) (2.35.0)\n","Requirement already satisfied: google-ai-generativelanguage==0.6.10 in c:\\python312\\lib\\site-packages (from google-generativeai) (0.6.10)\n","Requirement already satisfied: google-api-python-client in c:\\python312\\lib\\site-packages (from google-generativeai) (2.149.0)\n","Requirement already satisfied: pydantic in c:\\python312\\lib\\site-packages (from google-generativeai) (2.9.2)\n","Requirement already satisfied: tqdm in c:\\python312\\lib\\site-packages (from google-generativeai) (4.66.5)\n","Requirement already satisfied: typing-extensions in c:\\python312\\lib\\site-packages (from google-generativeai) (4.12.2)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\python312\\lib\\site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core) (5.5.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\python312\\lib\\site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core) (0.4.1)\n","Requirement already satisfied: rsa<5,>=3.1.4 in c:\\python312\\lib\\site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core) (4.9)\n","Requirement already satisfied: MarkupSafe>=2.0 in c:\\python312\\lib\\site-packages (from jinja2>=2.9->folium) (3.0.2)\n","Requirement already satisfied: six>=1.5 in c:\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n","Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in c:\\python312\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.22.0)\n","Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in c:\\python312\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.2.0)\n","Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\python312\\lib\\site-packages (from google-api-python-client->google-generativeai) (4.1.1)\n","Requirement already satisfied: annotated-types>=0.6.0 in c:\\python312\\lib\\site-packages (from pydantic->google-generativeai) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.23.4 in c:\\python312\\lib\\site-packages (from pydantic->google-generativeai) (2.23.4)\n","Requirement already satisfied: colorama in c:\\users\\shravan\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->google-generativeai) (0.4.6)\n","Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in c:\\python312\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai) (1.67.0)\n","Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in c:\\python312\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai) (1.67.0)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\python312\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core) (0.6.1)\n","Output directory already exists: ./maps\n","Data loaded successfully.\n","  Timestamp Report Number       City Crime Code Crime Description Victim Age  \\\n","0                       1  Ahmedabad        576    IDENTITY THEFT         16   \n","1                       2    Chennai        128          HOMICIDE         37   \n","2                       3   Ludhiana        271        KIDNAPPING         48   \n","3                       4       Pune        170          BURGLARY         49   \n","4                       5       Pune        421         VANDALISM         30   \n","\n","  Victim Gender   Weapon Used   Crime Domain Police Deployed Case Closed  \n","0             M  Blunt Object  Violent Crime              13          No  \n","1             M        Poison    Other Crime               9          No  \n","2             F  Blunt Object    Other Crime              15          No  \n","3             F       Firearm    Other Crime               1         Yes  \n","4             F         Other    Other Crime              18         Yes  \n","\n","Columns in the dataset:\n","Index(['Timestamp', 'Report Number', 'City', 'Crime Code', 'Crime Description',\n","       'Victim Age', 'Victim Gender', 'Weapon Used', 'Crime Domain',\n","       'Police Deployed', 'Case Closed'],\n","      dtype='object')\n","\n","Missing values:\n","Timestamp            0\n","Report Number        0\n","City                 0\n","Crime Code           0\n","Crime Description    0\n","Victim Age           0\n","Victim Gender        0\n","Weapon Used          0\n","Crime Domain         0\n","Police Deployed      0\n","Case Closed          0\n","dtype: int64\n","Heatmap saved to: ./maps\\crime_heatmap.html\n","Top 10 crime cities plot saved to: ./maps\\top_10_crime_cities.png\n","Top 10 crime types plot saved to: ./maps\\top_10_crime_types.png\n","Improved victim age distribution plot saved to: ./maps\\victim_age_distribution_improved.png\n","Analysis complete. Check ./maps for available visualizations.\n"]},{"data":{"text/html":["\n","        <iframe\n","            width=\"800\"\n","            height=\"600\"\n","            src=\"maps/crime_heatmap.html\"\n","            frameborder=\"0\"\n","            allowfullscreen\n","            \n","        ></iframe>\n","        "],"text/plain":["<IPython.lib.display.IFrame at 0x2b3b3c5cbc0>"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["!pip install pandas folium matplotlib seaborn numpy requests google-api-core google-generativeai\n","\n","import os\n","import sys\n","import pandas as pd\n","import folium\n","from folium.plugins import HeatMap\n","import matplotlib.pyplot as plt\n","import time\n","import json\n","from google.api_core import retry\n","import google.generativeai as genai\n","import seaborn as sns\n","import numpy as np\n","import requests\n","\n","# Define the output directory\n","output_dir = r'./maps'\n","\n","# Step 1: Create the output directory if it doesn't exist\n","if not os.path.exists(output_dir):\n","    try:\n","        os.makedirs(output_dir)\n","        print(f\"Output directory created: {output_dir}\")\n","    except Exception as e:\n","        print(f\"Error creating output directory: {e}\")\n","        sys.exit(1)\n","else:\n","    print(f\"Output directory already exists: {output_dir}\")\n","\n","# Step 2: Load the dataset from SheetDB API\n","API_URL = \"https://sheetdb.io/api/v1/uvztmda65pyb0\"\n","\n","def get_data_from_sheetdb():\n","    response = requests.get(API_URL)\n","    if response.status_code == 200:\n","        return pd.DataFrame(response.json())\n","    else:\n","        print(f\"Error fetching data: {response.status_code}\")\n","        sys.exit(1)\n","\n","data = get_data_from_sheetdb()\n","print(\"Data loaded successfully.\")\n","print(data.head())\n","\n","# Display column names and check for missing values\n","print(\"\\nColumns in the dataset:\")\n","print(data.columns)\n","\n","print(\"\\nMissing values:\")\n","print(data.isnull().sum())\n","\n","# Step 3: Add latitude and longitude\n","# Gemini API Key\n","GOOGLE_API_KEY = \"AIzaSyATXE22og8-HoroqLF9J5wlb1l58aHOhhU\"\n","\n","# Configure Google Generative AI\n","genai.configure(api_key=GOOGLE_API_KEY)\n","model = genai.GenerativeModel('gemini-pro')\n","\n","# Implement caching\n","cache_file = 'city_coordinates_cache.json'\n","try:\n","    with open(cache_file, 'r') as f:\n","        city_coordinates = json.load(f)\n","except FileNotFoundError:\n","    city_coordinates = {}\n","\n","@retry.Retry(predicate=retry.if_exception_type(Exception))\n","def get_gemini_response(question):\n","    response = model.generate_content(question)\n","    return response.text\n","\n","def get_lat_long_from_gemini(city_name):\n","    \"\"\"Function to get latitude and longitude from Gemini API with caching and rate limiting\"\"\"\n","    if city_name in city_coordinates:\n","        return city_coordinates[city_name]\n","\n","    try:\n","        response = get_gemini_response(f\"Provide only the latitude and longitude coordinates for {city_name}, India. Format the response as two decimal numbers separated by a comma.\")\n","        lat, lon = map(float, response.split(','))\n","        city_coordinates[city_name] = (lat, lon)\n","        \n","        # Save updated cache\n","        with open(cache_file, 'w') as f:\n","            json.dump(city_coordinates, f)\n","        \n","        return lat, lon\n","    except Exception as e:\n","        print(f\"Error fetching coordinates for {city_name}: {e}\")\n","        return None, None\n","\n","# Populate the city_coordinates dictionary\n","for city in data['City'].unique():\n","    if city not in city_coordinates:\n","        lat, lon = get_lat_long_from_gemini(city)\n","        if lat and lon:\n","            city_coordinates[city] = (lat, lon)\n","        time.sleep(1)  # Add a 1-second delay between API calls\n","\n","# Add Latitude and Longitude columns\n","data['Latitude'] = data['City'].map(lambda x: city_coordinates.get(x, (None, None))[0])\n","data['Longitude'] = data['City'].map(lambda x: city_coordinates.get(x, (None, None))[1])\n","\n","# Calculate Crime Rate (assuming it's based on the number of crimes per city)\n","crime_counts = data['City'].value_counts()\n","data['Crime Rate'] = data['City'].map(crime_counts)\n","\n","# Prepare data for HeatMap\n","heat_data = [[row['Latitude'], row['Longitude'], row['Crime Rate']] for index, row in data.iterrows() if pd.notnull(row['Latitude']) and pd.notnull(row['Longitude'])]\n","\n","# Create a base map centered on India\n","m = folium.Map(location=[20.5937, 78.9629], zoom_start=5)\n","HeatMap(heat_data).add_to(m)\n","\n","# Add HeatMap layer\n","HeatMap(heat_data).add_to(m)\n","\n","# Step 4: Save the map to an HTML file\n","try:\n","    map_path = os.path.join(output_dir, 'crime_heatmap.html')\n","    m.save(map_path)\n","    print(f\"Heatmap saved to: {map_path}\")\n","except Exception as e:\n","    print(f\"Error saving heatmap: {e}\")\n","\n","# Step 5: Identify top 10 cities with highest crime rates\n","top_10_cities = data.groupby('City')['Crime Rate'].mean().nlargest(10).reset_index()\n","\n","# Create a bar plot for top 10 cities\n","plt.figure(figsize=(12, 6))\n","plt.bar(top_10_cities['City'], top_10_cities['Crime Rate'])\n","plt.title('Top 10 Cities with Highest Crime Rates')\n","plt.xlabel('City')\n","plt.ylabel('Crime Rate')\n","plt.xticks(rotation=45, ha='right')\n","plt.tight_layout()\n","\n","# Step 6: Save the top 10 cities plot\n","try:\n","    cities_path = os.path.join(output_dir, 'top_10_crime_cities.png')\n","    plt.savefig(cities_path)\n","    plt.close()\n","    print(f\"Top 10 crime cities plot saved to: {cities_path}\")\n","except Exception as e:\n","    print(f\"Error saving top 10 crime cities plot: {e}\")\n","\n","# Step 7: Additional analysis based on the new data structure\n","# Crime type distribution\n","crime_type_dist = data['Crime Description'].value_counts().nlargest(10)\n","plt.figure(figsize=(12, 6))\n","crime_type_dist.plot(kind='bar')\n","plt.title('Top 10 Most Common Crime Types')\n","plt.xlabel('Crime Type')\n","plt.ylabel('Count')\n","plt.xticks(rotation=45, ha='right')\n","plt.tight_layout()\n","\n","try:\n","    crime_type_path = os.path.join(output_dir, 'top_10_crime_types.png')\n","    plt.savefig(crime_type_path)\n","    plt.close()\n","    print(f\"Top 10 crime types plot saved to: {crime_type_path}\")\n","except Exception as e:\n","    print(f\"Error saving top 10 crime types plot: {e}\")\n","\n","# Victim age distribution (improved version)\n","plt.figure(figsize=(12, 8))\n","sns.histplot(data['Victim Age'].astype(float), bins=20, kde=True, color=\"skyblue\", edgecolor=\"darkblue\")\n","\n","plt.title(\"Distribution of Victim Ages\", fontsize=16, fontweight='bold')\n","plt.xlabel(\"Age\", fontsize=12)\n","plt.ylabel(\"Frequency\", fontsize=12)\n","\n","# Add summary statistics\n","mean_age = data['Victim Age'].astype(float).mean()\n","median_age = data['Victim Age'].astype(float).median()\n","plt.axvline(mean_age, color='red', linestyle='dashed', linewidth=2, label=f'Mean Age: {mean_age:.1f}')\n","plt.axvline(median_age, color='green', linestyle='dashed', linewidth=2, label=f'Median Age: {median_age:.1f}')\n","\n","plt.legend(fontsize=10)\n","\n","# Add text for additional statistics\n","plt.text(0.95, 0.95, f\"Total Victims: {len(data)}\\nStd Dev: {data['Victim Age'].astype(float).std():.2f}\", \n","         transform=plt.gca().transAxes, ha='right', va='top', \n","         bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8))\n","\n","plt.tight_layout()\n","\n","try:\n","    age_dist_path = os.path.join(output_dir, 'victim_age_distribution_improved.png')\n","    plt.savefig(age_dist_path)\n","    plt.close()\n","    print(f\"Improved victim age distribution plot saved to: {age_dist_path}\")\n","except Exception as e:\n","    print(f\"Error saving improved victim age distribution plot: {e}\")\n","\n","print(f\"Analysis complete. Check {output_dir} for available visualizations.\")\n","import os\n","os.getcwd()\n","\n","from IPython.display import IFrame\n","IFrame(src=\"maps/crime_heatmap.html\", width=800, height=600)\n"]},{"cell_type":"markdown","id":"9b608422","metadata":{},"source":[]},{"cell_type":"code","execution_count":9,"id":"9087230e-77d7-4220-9e51-e48db75d9fad","metadata":{"execution":{"iopub.execute_input":"2024-10-20T02:30:26.226751Z","iopub.status.busy":"2024-10-20T02:30:26.226341Z","iopub.status.idle":"2024-10-20T02:30:28.602565Z","shell.execute_reply":"2024-10-20T02:30:28.601355Z","shell.execute_reply.started":"2024-10-20T02:30:26.226706Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading crime data...\n","Available columns in the dataset:\n","['Timestamp', 'Report Number', 'City', 'Crime Code', 'Crime Description', 'Victim Age', 'Victim Gender', 'Weapon Used', 'Crime Domain', 'Police Deployed', 'Case Closed']\n","\n","Data shape: (40161, 11)\n","\n","Sample data:\n","  Timestamp  Report Number       City  Crime Code Crime Description  \\\n","0   Unknown              1  Ahmedabad         576    IDENTITY THEFT   \n","1   Unknown              2    Chennai         128          HOMICIDE   \n","2   Unknown              3   Ludhiana         271        KIDNAPPING   \n","3   Unknown              4       Pune         170          BURGLARY   \n","4   Unknown              5       Pune         421         VANDALISM   \n","\n","   Victim Age Victim Gender   Weapon Used   Crime Domain  Police Deployed  \\\n","0          16             M  Blunt Object  Violent Crime               13   \n","1          37             M        Poison    Other Crime                9   \n","2          48             F  Blunt Object    Other Crime               15   \n","3          49             F       Firearm    Other Crime                1   \n","4          30             F         Other    Other Crime               18   \n","\n","  Case Closed  \n","0          No  \n","1          No  \n","2          No  \n","3         Yes  \n","4         Yes  \n","\n","Dataset Info:\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 40161 entries, 0 to 40160\n","Data columns (total 11 columns):\n"," #   Column             Non-Null Count  Dtype \n","---  ------             --------------  ----- \n"," 0   Timestamp          40161 non-null  object\n"," 1   Report Number      40161 non-null  int64 \n"," 2   City               40161 non-null  object\n"," 3   Crime Code         40161 non-null  int64 \n"," 4   Crime Description  40161 non-null  object\n"," 5   Victim Age         40161 non-null  int64 \n"," 6   Victim Gender      40161 non-null  object\n"," 7   Weapon Used        40161 non-null  object\n"," 8   Crime Domain       40161 non-null  object\n"," 9   Police Deployed    40161 non-null  int64 \n"," 10  Case Closed        40161 non-null  object\n","dtypes: int64(4), object(7)\n","memory usage: 3.4+ MB\n","None\n","\n","Using columns:\n","Time column: Timestamp\n","Crime type column: Crime Description\n","City column: City\n","\n","Class distribution:\n","Crime Description\n","BURGLARY               1980\n","VANDALISM              1976\n","FRAUD                  1965\n","DOMESTIC VIOLENCE      1932\n","FIREARM OFFENSE        1931\n","ROBBERY                1928\n","KIDNAPPING             1920\n","IDENTITY THEFT         1918\n","SEXUAL ASSAULT         1917\n","ASSAULT                1915\n","TRAFFIC VIOLATION      1915\n","PUBLIC INTOXICATION    1912\n","HOMICIDE               1909\n","CYBERCRIME             1899\n","ILLEGAL POSSESSION     1895\n","ARSON                  1894\n","DRUG OFFENSE           1890\n","EXTORTION              1873\n","COUNTERFEITING         1871\n","VEHICLE - STOLEN       1862\n","SHOPLIFTING            1859\n","Name: count, dtype: int64\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"ename":"UnboundLocalError","evalue":"local variable 'device' referenced before assignment","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[9], line 294\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28mprint\u001b[39m(data\u001b[38;5;241m.\u001b[39mhead())\n\u001b[1;32m    293\u001b[0m \u001b[38;5;66;03m# Set up and train the prediction model\u001b[39;00m\n\u001b[0;32m--> 294\u001b[0m model, tokenizer, label_encoder, device \u001b[38;5;241m=\u001b[39m \u001b[43msetup_crime_prediction_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;66;03m# Save the trained model\u001b[39;00m\n\u001b[1;32m    297\u001b[0m model_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcrime_prediction_model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","Cell \u001b[0;32mIn[9], line 209\u001b[0m, in \u001b[0;36msetup_crime_prediction_model\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    207\u001b[0m class_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m/\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(class_counts, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[1;32m    208\u001b[0m class_weights \u001b[38;5;241m=\u001b[39m class_weights \u001b[38;5;241m/\u001b[39m class_weights\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m--> 209\u001b[0m class_weights \u001b[38;5;241m=\u001b[39m class_weights\u001b[38;5;241m.\u001b[39mto(\u001b[43mdevice\u001b[49m)\n\u001b[1;32m    211\u001b[0m \u001b[38;5;66;03m# Add class weights to the model's config\u001b[39;00m\n\u001b[1;32m    212\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mclass_weights \u001b[38;5;241m=\u001b[39m class_weights\u001b[38;5;241m.\u001b[39mtolist()\n","\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'device' referenced before assignment"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n","from torch.utils.data import Dataset, DataLoader\n","import torch\n","from sklearn.metrics import classification_report\n","import requests\n","import os\n","\n","def load_crime_data():\n","    \"\"\"Load crime data from the Google Sheets URL\"\"\"\n","    # Google Sheets URL to CSV export\n","    SHEET_ID = \"1Pjtr4PO1BOBiQRRZoJNbin2RBuFLmy-ryZSx7WtK0CA\"\n","    SHEET_NAME = \"Form responses 1\"\n","    url = f\"https://docs.google.com/spreadsheets/d/{SHEET_ID}/export?format=csv&gid=630647404\"\n","    \n","    try:\n","        # Read the CSV data directly from the URL\n","        df = pd.read_csv(url)\n","        \n","        # Print available columns for debugging\n","        print(\"Available columns in the dataset:\")\n","        print(df.columns.tolist())\n","        \n","        # Basic data cleaning\n","        # Remove any completely empty rows\n","        df = df.dropna(how='all')\n","        \n","        # Fill missing values for text columns with 'Unknown'\n","        text_columns = df.select_dtypes(include=['object']).columns\n","        df[text_columns] = df[text_columns].fillna('Unknown')\n","        \n","        return df\n","    \n","    except Exception as e:\n","        print(f\"Error loading data: {e}\")\n","        raise\n","\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from transformers import BertTokenizer, BertForSequenceClassification\n","from torch.optim import AdamW  # Updated to use PyTorch's AdamW\n","from torch.utils.data import Dataset, DataLoader\n","import torch\n","from sklearn.metrics import classification_report\n","import requests\n","import os\n","\n","class CrimeDataset(Dataset):\n","    def __init__(self, texts, labels, tokenizer, max_len=128):\n","        # Convert pandas Series to list to avoid indexing issues\n","        self.texts = texts.values if isinstance(texts, pd.Series) else texts\n","        self.labels = labels.values if isinstance(labels, pd.Series) else labels\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","    \n","    def __len__(self):\n","        return len(self.texts)\n","    \n","    def __getitem__(self, idx):\n","        text = str(self.texts[idx])\n","        label = self.labels[idx]\n","        \n","        encoding = self.tokenizer.encode_plus(\n","            text,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            return_token_type_ids=True,\n","            padding='max_length',\n","            truncation=True,\n","            return_attention_mask=True,\n","            return_tensors='pt'\n","        )\n","        \n","        return {\n","            'input_ids': encoding['input_ids'].flatten(),\n","            'attention_mask': encoding['attention_mask'].flatten(),\n","            'token_type_ids': encoding['token_type_ids'].flatten(),\n","            'labels': torch.tensor(label, dtype=torch.long)\n","        }\n","\n","def train_model(model, train_loader, val_loader, device, epochs=3):\n","    optimizer = AdamW(model.parameters(), lr=2e-5)\n","    \n","    for epoch in range(epochs):\n","        model.train()\n","        train_loss = 0\n","        for batch_idx, batch in enumerate(train_loader):\n","            optimizer.zero_grad()\n","            \n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            token_type_ids = batch['token_type_ids'].to(device)\n","            labels = batch['labels'].to(device)\n","            \n","            outputs = model(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","                token_type_ids=token_type_ids,\n","                labels=labels\n","            )\n","            \n","            loss = outputs.loss\n","            train_loss += loss.item()\n","            \n","            loss.backward()\n","            optimizer.step()\n","            \n","            if (batch_idx + 1) % 100 == 0:\n","                print(f'Epoch {epoch + 1}, Batch {batch_idx + 1}, Loss: {loss.item():.4f}')\n","        \n","        # Validation\n","        model.eval()\n","        val_loss = 0\n","        predictions = []\n","        actual_labels = []\n","        \n","        with torch.no_grad():\n","            for batch in val_loader:\n","                input_ids = batch['input_ids'].to(device)\n","                attention_mask = batch['attention_mask'].to(device)\n","                token_type_ids = batch['token_type_ids'].to(device)\n","                labels = batch['labels'].to(device)\n","                \n","                outputs = model(\n","                    input_ids=input_ids,\n","                    attention_mask=attention_mask,\n","                    token_type_ids=token_type_ids,\n","                    labels=labels\n","                )\n","                \n","                val_loss += outputs.loss.item()\n","                preds = torch.argmax(outputs.logits, dim=1)\n","                predictions.extend(preds.cpu().numpy())\n","                actual_labels.extend(labels.cpu().numpy())\n","        \n","        print(f'Epoch {epoch + 1}:')\n","        print(f'Average training loss: {train_loss / len(train_loader)}')\n","        print(f'Average validation loss: {val_loss / len(val_loader)}')\n","        print('Classification Report:')\n","        print(classification_report(\n","            actual_labels, \n","            predictions, \n","            zero_division=0,  # Handle zero-division case\n","            digits=4  # Increase precision in the report\n","        ))\n","\n","def setup_crime_prediction_model(data):\n","    \"\"\"Set up and train the BERT model with the available columns\"\"\"\n","    # Print data info for debugging\n","    print(\"\\nDataset Info:\")\n","    print(data.info())\n","    \n","    # Identify available columns for features\n","    time_col = 'Timestamp'\n","    crime_type_col = 'Crime Description'\n","    city_col = 'City'\n","    \n","    print(\"\\nUsing columns:\")\n","    print(f\"Time column: {time_col}\")\n","    print(f\"Crime type column: {crime_type_col}\")\n","    print(f\"City column: {city_col}\")\n","    \n","    # Prepare the features using available columns\n","    features = data[city_col].astype(str)\n","    if time_col in data.columns:\n","        features = features + ' ' + data[time_col].astype(str)\n","    \n","    if 'Crime Domain' in data.columns:\n","        features = features + ' ' + data['Crime Domain'].astype(str)\n","    \n","    # Prepare labels\n","    labels = data[crime_type_col]\n","    \n","    # Print class distribution\n","    print(\"\\nClass distribution:\")\n","    print(labels.value_counts())\n","    \n","    # Encode labels\n","    label_encoder = LabelEncoder()\n","    encoded_labels = label_encoder.fit_transform(labels)\n","    \n","    # Use stratified split to maintain class distribution\n","    X_train, X_val, y_train, y_val = train_test_split(\n","        features, \n","        encoded_labels, \n","        test_size=0.2, \n","        random_state=42,\n","        stratify=encoded_labels  # Ensure balanced split\n","    )\n","    \n","    # Initialize tokenizer and model\n","    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","    model = BertForSequenceClassification.from_pretrained(\n","        'bert-base-uncased',\n","        num_labels=len(label_encoder.classes_),\n","        # Add class weights to handle imbalance\n","        problem_type=\"single_label_classification\"\n","    )\n","    \n","    # Calculate class weights\n","    class_counts = np.bincount(encoded_labels)\n","    class_weights = 1. / torch.tensor(class_counts, dtype=torch.float)\n","    class_weights = class_weights / class_weights.sum()\n","    class_weights = class_weights.to(device)\n","    \n","    # Add class weights to the model's config\n","    model.config.class_weights = class_weights.tolist()\n","    \n","    # Create datasets with balanced sampling\n","    train_dataset = CrimeDataset(X_train, y_train, tokenizer)\n","    val_dataset = CrimeDataset(X_val, y_val, tokenizer)\n","    \n","    # Calculate sample weights for training data\n","    train_sample_weights = [class_weights[label].item() for label in y_train]\n","    \n","    # Create weighted sampler for training data\n","    train_sampler = torch.utils.data.WeightedRandomSampler(\n","        weights=train_sample_weights,\n","        num_samples=len(train_sample_weights),\n","        replacement=True\n","    )\n","    \n","    # Create data loaders with weighted sampler for training\n","    train_loader = DataLoader(\n","        train_dataset,\n","        batch_size=16,\n","        sampler=train_sampler,  # Use weighted sampler\n","        num_workers=0\n","    )\n","    val_loader = DataLoader(\n","        val_dataset,\n","        batch_size=16,\n","        shuffle=False,\n","        num_workers=0\n","    )\n","    \n","    # Set device\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    model = model.to(device)\n","    \n","    # Train the model\n","    print(\"\\nTraining the crime prediction model...\")\n","    train_model(model, train_loader, val_loader, device)\n","    \n","    return model, tokenizer, label_encoder, device\n","\n","def predict_crime_type(text, model, tokenizer, label_encoder, device):\n","    model.eval()\n","    encoding = tokenizer.encode_plus(\n","        text,\n","        add_special_tokens=True,\n","        max_length=128,\n","        return_token_type_ids=True,\n","        padding='max_length',\n","        truncation=True,\n","        return_attention_mask=True,\n","        return_tensors='pt'\n","    )\n","    \n","    with torch.no_grad():\n","        input_ids = encoding['input_ids'].to(device)\n","        attention_mask = encoding['attention_mask'].to(device)\n","        token_type_ids = encoding['token_type_ids'].to(device)\n","        \n","        outputs = model(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids\n","        )\n","        \n","        probs = torch.softmax(outputs.logits, dim=1)\n","        predicted_class = torch.argmax(probs, dim=1)\n","        \n","    return label_encoder.inverse_transform(predicted_class.cpu().numpy())[0], probs[0][predicted_class].item()\n","\n","if __name__ == \"__main__\":\n","    # Define the output directory\n","    output_dir = 'crime_prediction_output'\n","    os.makedirs(output_dir, exist_ok=True)\n","    \n","    # Load the data and print column names\n","    print(\"Loading crime data...\")\n","    data = load_crime_data()\n","    print(\"\\nData shape:\", data.shape)\n","    print(\"\\nSample data:\")\n","    print(data.head())\n","    \n","    # Set up and train the prediction model\n","    model, tokenizer, label_encoder, device = setup_crime_prediction_model(data)\n","    \n","    # Save the trained model\n","    model_path = os.path.join(output_dir, 'crime_prediction_model')\n","    os.makedirs(model_path, exist_ok=True)\n","    model.save_pretrained(model_path)\n","    tokenizer.save_pretrained(model_path)\n","    \n","    # Example prediction\n","    sample_input = \"Mumbai Andheri West 14:00\"\n","    predicted_crime, confidence = predict_crime_type(\n","        sample_input, model, tokenizer, label_encoder, device\n","    )\n","    print(f\"\\nSample Prediction:\")\n","    print(f\"Input: {sample_input}\")\n","    print(f\"Predicted Crime Type: {predicted_crime}\")\n","    print(f\"Confidence: {confidence:.2%}\")"]}],"metadata":{"colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4388831,"sourceId":7536591,"sourceType":"datasetVersion"}],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"},"papermill":{"default_parameters":{},"duration":38.428223,"end_time":"2024-02-02T06:23:23.410637","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-02-02T06:22:44.982414","version":"2.5.0"}},"nbformat":4,"nbformat_minor":5}
