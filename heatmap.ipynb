{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7536591,"sourceType":"datasetVersion","datasetId":4388831}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[]},"papermill":{"default_parameters":{},"duration":38.428223,"end_time":"2024-02-02T06:23:23.410637","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-02-02T06:22:44.982414","version":"2.5.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"996bca8a","cell_type":"code","source":"import os\nimport sys\nimport pandas as pd\nimport folium\nfrom folium.plugins import HeatMap\nimport matplotlib.pyplot as plt\nimport time\nimport json\nfrom google.api_core import retry\nimport google.generativeai as genai\nimport seaborn as sns\nimport numpy as np\nimport requests\n\n# Define the output directory\noutput_dir = r'/kaggle/working/maps'\n\n# Step 1: Create the output directory if it doesn't exist\nif not os.path.exists(output_dir):\n    try:\n        os.makedirs(output_dir)\n        print(f\"Output directory created: {output_dir}\")\n    except Exception as e:\n        print(f\"Error creating output directory: {e}\")\n        sys.exit(1)\nelse:\n    print(f\"Output directory already exists: {output_dir}\")\n\n# Step 2: Load the dataset from SheetDB API\nAPI_URL = \"https://sheetdb.io/api/v1/uvztmda65pyb0\"\n\ndef get_data_from_sheetdb():\n    response = requests.get(API_URL)\n    if response.status_code == 200:\n        return pd.DataFrame(response.json())\n    else:\n        print(f\"Error fetching data: {response.status_code}\")\n        sys.exit(1)\n\ndata = get_data_from_sheetdb()\nprint(\"Data loaded successfully.\")\nprint(data.head())\n\n# Display column names and check for missing values\nprint(\"\\nColumns in the dataset:\")\nprint(data.columns)\n\nprint(\"\\nMissing values:\")\nprint(data.isnull().sum())\n\n# Step 3: Add latitude and longitude\n# Gemini API Key\nGOOGLE_API_KEY = \"AIzaSyATXE22og8-HoroqLF9J5wlb1l58aHOhhU\"\n\n# Configure Google Generative AI\ngenai.configure(api_key=GOOGLE_API_KEY)\nmodel = genai.GenerativeModel('gemini-pro')\n\n# Implement caching\ncache_file = 'city_coordinates_cache.json'\ntry:\n    with open(cache_file, 'r') as f:\n        city_coordinates = json.load(f)\nexcept FileNotFoundError:\n    city_coordinates = {}\n\n@retry.Retry(predicate=retry.if_exception_type(Exception))\ndef get_gemini_response(question):\n    response = model.generate_content(question)\n    return response.text\n\ndef get_lat_long_from_gemini(city_name):\n    \"\"\"Function to get latitude and longitude from Gemini API with caching and rate limiting\"\"\"\n    if city_name in city_coordinates:\n        return city_coordinates[city_name]\n\n    try:\n        response = get_gemini_response(f\"Provide only the latitude and longitude coordinates for {city_name}, India. Format the response as two decimal numbers separated by a comma.\")\n        lat, lon = map(float, response.split(','))\n        city_coordinates[city_name] = (lat, lon)\n        \n        # Save updated cache\n        with open(cache_file, 'w') as f:\n            json.dump(city_coordinates, f)\n        \n        return lat, lon\n    except Exception as e:\n        print(f\"Error fetching coordinates for {city_name}: {e}\")\n        return None, None\n\n# Populate the city_coordinates dictionary\nfor city in data['City'].unique():\n    if city not in city_coordinates:\n        lat, lon = get_lat_long_from_gemini(city)\n        if lat and lon:\n            city_coordinates[city] = (lat, lon)\n        time.sleep(1)  # Add a 1-second delay between API calls\n\n# Add Latitude and Longitude columns\ndata['Latitude'] = data['City'].map(lambda x: city_coordinates.get(x, (None, None))[0])\ndata['Longitude'] = data['City'].map(lambda x: city_coordinates.get(x, (None, None))[1])\n\n# Calculate Crime Rate (assuming it's based on the number of crimes per city)\ncrime_counts = data['City'].value_counts()\ndata['Crime Rate'] = data['City'].map(crime_counts)\n\n# Prepare data for HeatMap\nheat_data = [[row['Latitude'], row['Longitude'], row['Crime Rate']] for index, row in data.iterrows() if pd.notnull(row['Latitude']) and pd.notnull(row['Longitude'])]\n\n# Create a base map centered on India\nm = folium.Map(location=[20.5937, 78.9629], zoom_start=5)\nHeatMap(heat_data).add_to(m)\n\n# Add HeatMap layer\nHeatMap(heat_data).add_to(m)\n\n# Step 4: Save the map to an HTML file\ntry:\n    map_path = os.path.join(output_dir, 'crime_heatmap.html')\n    m.save(map_path)\n    print(f\"Heatmap saved to: {map_path}\")\nexcept Exception as e:\n    print(f\"Error saving heatmap: {e}\")\n\n# Step 5: Identify top 10 cities with highest crime rates\ntop_10_cities = data.groupby('City')['Crime Rate'].mean().nlargest(10).reset_index()\n\n# Create a bar plot for top 10 cities\nplt.figure(figsize=(12, 6))\nplt.bar(top_10_cities['City'], top_10_cities['Crime Rate'])\nplt.title('Top 10 Cities with Highest Crime Rates')\nplt.xlabel('City')\nplt.ylabel('Crime Rate')\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\n\n# Step 6: Save the top 10 cities plot\ntry:\n    cities_path = os.path.join(output_dir, 'top_10_crime_cities.png')\n    plt.savefig(cities_path)\n    plt.close()\n    print(f\"Top 10 crime cities plot saved to: {cities_path}\")\nexcept Exception as e:\n    print(f\"Error saving top 10 crime cities plot: {e}\")\n\n# Step 7: Additional analysis based on the new data structure\n# Crime type distribution\ncrime_type_dist = data['Crime Description'].value_counts().nlargest(10)\nplt.figure(figsize=(12, 6))\ncrime_type_dist.plot(kind='bar')\nplt.title('Top 10 Most Common Crime Types')\nplt.xlabel('Crime Type')\nplt.ylabel('Count')\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\n\ntry:\n    crime_type_path = os.path.join(output_dir, 'top_10_crime_types.png')\n    plt.savefig(crime_type_path)\n    plt.close()\n    print(f\"Top 10 crime types plot saved to: {crime_type_path}\")\nexcept Exception as e:\n    print(f\"Error saving top 10 crime types plot: {e}\")\n\n# Victim age distribution (improved version)\nplt.figure(figsize=(12, 8))\nsns.histplot(data['Victim Age'].astype(float), bins=20, kde=True, color=\"skyblue\", edgecolor=\"darkblue\")\n\nplt.title(\"Distribution of Victim Ages\", fontsize=16, fontweight='bold')\nplt.xlabel(\"Age\", fontsize=12)\nplt.ylabel(\"Frequency\", fontsize=12)\n\n# Add summary statistics\nmean_age = data['Victim Age'].astype(float).mean()\nmedian_age = data['Victim Age'].astype(float).median()\nplt.axvline(mean_age, color='red', linestyle='dashed', linewidth=2, label=f'Mean Age: {mean_age:.1f}')\nplt.axvline(median_age, color='green', linestyle='dashed', linewidth=2, label=f'Median Age: {median_age:.1f}')\n\nplt.legend(fontsize=10)\n\n# Add text for additional statistics\nplt.text(0.95, 0.95, f\"Total Victims: {len(data)}\\nStd Dev: {data['Victim Age'].astype(float).std():.2f}\", \n         transform=plt.gca().transAxes, ha='right', va='top', \n         bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8))\n\nplt.tight_layout()\n\ntry:\n    age_dist_path = os.path.join(output_dir, 'victim_age_distribution_improved.png')\n    plt.savefig(age_dist_path)\n    plt.close()\n    print(f\"Improved victim age distribution plot saved to: {age_dist_path}\")\nexcept Exception as e:\n    print(f\"Error saving improved victim age distribution plot: {e}\")\n\nprint(f\"Analysis complete. Check {output_dir} for available visualizations.\")\nimport os\nos.getcwd()\n\nfrom IPython.display import IFrame\nIFrame(src=\"/kaggle/working/maps/crime_heatmap.html\", width=800, height=600)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-20T02:30:17.355382Z","iopub.execute_input":"2024-10-20T02:30:17.356044Z","iopub.status.idle":"2024-10-20T02:30:26.224256Z","shell.execute_reply.started":"2024-10-20T02:30:17.356008Z","shell.execute_reply":"2024-10-20T02:30:26.223293Z"}},"outputs":[{"name":"stdout","text":"Output directory already exists: /kaggle/working/maps\nData loaded successfully.\n  Timestamp Report Number       City Crime Code Crime Description Victim Age  \\\n0                       1  Ahmedabad        576    IDENTITY THEFT         16   \n1                       2    Chennai        128          HOMICIDE         37   \n2                       3   Ludhiana        271        KIDNAPPING         48   \n3                       4       Pune        170          BURGLARY         49   \n4                       5       Pune        421         VANDALISM         30   \n\n  Victim Gender   Weapon Used   Crime Domain Police Deployed Case Closed  \n0             M  Blunt Object  Violent Crime              13          No  \n1             M        Poison    Other Crime               9          No  \n2             F  Blunt Object    Other Crime              15          No  \n3             F       Firearm    Other Crime               1         Yes  \n4             F         Other    Other Crime              18         Yes  \n\nColumns in the dataset:\nIndex(['Timestamp', 'Report Number', 'City', 'Crime Code', 'Crime Description',\n       'Victim Age', 'Victim Gender', 'Weapon Used', 'Crime Domain',\n       'Police Deployed', 'Case Closed'],\n      dtype='object')\n\nMissing values:\nTimestamp            0\nReport Number        0\nCity                 0\nCrime Code           0\nCrime Description    0\nVictim Age           0\nVictim Gender        0\nWeapon Used          0\nCrime Domain         0\nPolice Deployed      0\nCase Closed          0\ndtype: int64\nHeatmap saved to: /kaggle/working/maps/crime_heatmap.html\nTop 10 crime cities plot saved to: /kaggle/working/maps/top_10_crime_cities.png\nTop 10 crime types plot saved to: /kaggle/working/maps/top_10_crime_types.png\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n","output_type":"stream"},{"name":"stdout","text":"Improved victim age distribution plot saved to: /kaggle/working/maps/victim_age_distribution_improved.png\nAnalysis complete. Check /kaggle/working/maps for available visualizations.\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"<IPython.lib.display.IFrame at 0x7ec634cd71c0>","text/html":"\n        <iframe\n            width=\"800\"\n            height=\"600\"\n            src=\"/kaggle/working/maps/crime_heatmap.html\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        "},"metadata":{}}],"execution_count":8},{"id":"9087230e-77d7-4220-9e51-e48db75d9fad","cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nfrom sklearn.metrics import classification_report\nimport requests\nimport os\n\ndef load_crime_data():\n    \"\"\"Load crime data from the Google Sheets URL\"\"\"\n    # Google Sheets URL to CSV export\n    SHEET_ID = \"1Pjtr4PO1BOBiQRRZoJNbin2RBuFLmy-ryZSx7WtK0CA\"\n    SHEET_NAME = \"Form responses 1\"\n    url = f\"https://docs.google.com/spreadsheets/d/{SHEET_ID}/export?format=csv&gid=630647404\"\n    \n    try:\n        # Read the CSV data directly from the URL\n        df = pd.read_csv(url)\n        \n        # Print available columns for debugging\n        print(\"Available columns in the dataset:\")\n        print(df.columns.tolist())\n        \n        # Basic data cleaning\n        # Remove any completely empty rows\n        df = df.dropna(how='all')\n        \n        # Fill missing values for text columns with 'Unknown'\n        text_columns = df.select_dtypes(include=['object']).columns\n        df[text_columns] = df[text_columns].fillna('Unknown')\n        \n        return df\n    \n    except Exception as e:\n        print(f\"Error loading data: {e}\")\n        raise\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom torch.optim import AdamW  # Updated to use PyTorch's AdamW\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nfrom sklearn.metrics import classification_report\nimport requests\nimport os\n\nclass CrimeDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len=128):\n        # Convert pandas Series to list to avoid indexing issues\n        self.texts = texts.values if isinstance(texts, pd.Series) else texts\n        self.labels = labels.values if isinstance(labels, pd.Series) else labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        label = self.labels[idx]\n        \n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=True,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n        \n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'token_type_ids': encoding['token_type_ids'].flatten(),\n            'labels': torch.tensor(label, dtype=torch.long)\n        }\n\ndef train_model(model, train_loader, val_loader, device, epochs=3):\n    optimizer = AdamW(model.parameters(), lr=2e-5)\n    \n    for epoch in range(epochs):\n        model.train()\n        train_loss = 0\n        for batch_idx, batch in enumerate(train_loader):\n            optimizer.zero_grad()\n            \n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            token_type_ids = batch['token_type_ids'].to(device)\n            labels = batch['labels'].to(device)\n            \n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                token_type_ids=token_type_ids,\n                labels=labels\n            )\n            \n            loss = outputs.loss\n            train_loss += loss.item()\n            \n            loss.backward()\n            optimizer.step()\n            \n            if (batch_idx + 1) % 100 == 0:\n                print(f'Epoch {epoch + 1}, Batch {batch_idx + 1}, Loss: {loss.item():.4f}')\n        \n        # Validation\n        model.eval()\n        val_loss = 0\n        predictions = []\n        actual_labels = []\n        \n        with torch.no_grad():\n            for batch in val_loader:\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                token_type_ids = batch['token_type_ids'].to(device)\n                labels = batch['labels'].to(device)\n                \n                outputs = model(\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    token_type_ids=token_type_ids,\n                    labels=labels\n                )\n                \n                val_loss += outputs.loss.item()\n                preds = torch.argmax(outputs.logits, dim=1)\n                predictions.extend(preds.cpu().numpy())\n                actual_labels.extend(labels.cpu().numpy())\n        \n        print(f'Epoch {epoch + 1}:')\n        print(f'Average training loss: {train_loss / len(train_loader)}')\n        print(f'Average validation loss: {val_loss / len(val_loader)}')\n        print('Classification Report:')\n        print(classification_report(\n            actual_labels, \n            predictions, \n            zero_division=0,  # Handle zero-division case\n            digits=4  # Increase precision in the report\n        ))\n\ndef setup_crime_prediction_model(data):\n    \"\"\"Set up and train the BERT model with the available columns\"\"\"\n    # Print data info for debugging\n    print(\"\\nDataset Info:\")\n    print(data.info())\n    \n    # Identify available columns for features\n    time_col = 'Timestamp'\n    crime_type_col = 'Crime Description'\n    city_col = 'City'\n    \n    print(\"\\nUsing columns:\")\n    print(f\"Time column: {time_col}\")\n    print(f\"Crime type column: {crime_type_col}\")\n    print(f\"City column: {city_col}\")\n    \n    # Prepare the features using available columns\n    features = data[city_col].astype(str)\n    if time_col in data.columns:\n        features = features + ' ' + data[time_col].astype(str)\n    \n    if 'Crime Domain' in data.columns:\n        features = features + ' ' + data['Crime Domain'].astype(str)\n    \n    # Prepare labels\n    labels = data[crime_type_col]\n    \n    # Print class distribution\n    print(\"\\nClass distribution:\")\n    print(labels.value_counts())\n    \n    # Encode labels\n    label_encoder = LabelEncoder()\n    encoded_labels = label_encoder.fit_transform(labels)\n    \n    # Use stratified split to maintain class distribution\n    X_train, X_val, y_train, y_val = train_test_split(\n        features, \n        encoded_labels, \n        test_size=0.2, \n        random_state=42,\n        stratify=encoded_labels  # Ensure balanced split\n    )\n    \n    # Initialize tokenizer and model\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n    model = BertForSequenceClassification.from_pretrained(\n        'bert-base-uncased',\n        num_labels=len(label_encoder.classes_),\n        # Add class weights to handle imbalance\n        problem_type=\"single_label_classification\"\n    )\n    \n    # Calculate class weights\n    class_counts = np.bincount(encoded_labels)\n    class_weights = 1. / torch.tensor(class_counts, dtype=torch.float)\n    class_weights = class_weights / class_weights.sum()\n    class_weights = class_weights.to(device)\n    \n    # Add class weights to the model's config\n    model.config.class_weights = class_weights.tolist()\n    \n    # Create datasets with balanced sampling\n    train_dataset = CrimeDataset(X_train, y_train, tokenizer)\n    val_dataset = CrimeDataset(X_val, y_val, tokenizer)\n    \n    # Calculate sample weights for training data\n    train_sample_weights = [class_weights[label].item() for label in y_train]\n    \n    # Create weighted sampler for training data\n    train_sampler = torch.utils.data.WeightedRandomSampler(\n        weights=train_sample_weights,\n        num_samples=len(train_sample_weights),\n        replacement=True\n    )\n    \n    # Create data loaders with weighted sampler for training\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=16,\n        sampler=train_sampler,  # Use weighted sampler\n        num_workers=0\n    )\n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=16,\n        shuffle=False,\n        num_workers=0\n    )\n    \n    # Set device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = model.to(device)\n    \n    # Train the model\n    print(\"\\nTraining the crime prediction model...\")\n    train_model(model, train_loader, val_loader, device)\n    \n    return model, tokenizer, label_encoder, device\n\ndef predict_crime_type(text, model, tokenizer, label_encoder, device):\n    model.eval()\n    encoding = tokenizer.encode_plus(\n        text,\n        add_special_tokens=True,\n        max_length=128,\n        return_token_type_ids=True,\n        padding='max_length',\n        truncation=True,\n        return_attention_mask=True,\n        return_tensors='pt'\n    )\n    \n    with torch.no_grad():\n        input_ids = encoding['input_ids'].to(device)\n        attention_mask = encoding['attention_mask'].to(device)\n        token_type_ids = encoding['token_type_ids'].to(device)\n        \n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids\n        )\n        \n        probs = torch.softmax(outputs.logits, dim=1)\n        predicted_class = torch.argmax(probs, dim=1)\n        \n    return label_encoder.inverse_transform(predicted_class.cpu().numpy())[0], probs[0][predicted_class].item()\n\nif __name__ == \"__main__\":\n    # Define the output directory\n    output_dir = 'crime_prediction_output'\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Load the data and print column names\n    print(\"Loading crime data...\")\n    data = load_crime_data()\n    print(\"\\nData shape:\", data.shape)\n    print(\"\\nSample data:\")\n    print(data.head())\n    \n    # Set up and train the prediction model\n    model, tokenizer, label_encoder, device = setup_crime_prediction_model(data)\n    \n    # Save the trained model\n    model_path = os.path.join(output_dir, 'crime_prediction_model')\n    os.makedirs(model_path, exist_ok=True)\n    model.save_pretrained(model_path)\n    tokenizer.save_pretrained(model_path)\n    \n    # Example prediction\n    sample_input = \"Mumbai Andheri West 14:00\"\n    predicted_crime, confidence = predict_crime_type(\n        sample_input, model, tokenizer, label_encoder, device\n    )\n    print(f\"\\nSample Prediction:\")\n    print(f\"Input: {sample_input}\")\n    print(f\"Predicted Crime Type: {predicted_crime}\")\n    print(f\"Confidence: {confidence:.2%}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-20T02:30:26.226341Z","iopub.execute_input":"2024-10-20T02:30:26.226751Z","iopub.status.idle":"2024-10-20T02:30:28.602565Z","shell.execute_reply.started":"2024-10-20T02:30:26.226706Z","shell.execute_reply":"2024-10-20T02:30:28.601355Z"}},"outputs":[{"name":"stdout","text":"Loading crime data...\nAvailable columns in the dataset:\n['Timestamp', 'Report Number', 'City', 'Crime Code', 'Crime Description', 'Victim Age', 'Victim Gender', 'Weapon Used', 'Crime Domain', 'Police Deployed', 'Case Closed']\n\nData shape: (40161, 11)\n\nSample data:\n  Timestamp  Report Number       City  Crime Code Crime Description  \\\n0   Unknown              1  Ahmedabad         576    IDENTITY THEFT   \n1   Unknown              2    Chennai         128          HOMICIDE   \n2   Unknown              3   Ludhiana         271        KIDNAPPING   \n3   Unknown              4       Pune         170          BURGLARY   \n4   Unknown              5       Pune         421         VANDALISM   \n\n   Victim Age Victim Gender   Weapon Used   Crime Domain  Police Deployed  \\\n0          16             M  Blunt Object  Violent Crime               13   \n1          37             M        Poison    Other Crime                9   \n2          48             F  Blunt Object    Other Crime               15   \n3          49             F       Firearm    Other Crime                1   \n4          30             F         Other    Other Crime               18   \n\n  Case Closed  \n0          No  \n1          No  \n2          No  \n3         Yes  \n4         Yes  \n\nDataset Info:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 40161 entries, 0 to 40160\nData columns (total 11 columns):\n #   Column             Non-Null Count  Dtype \n---  ------             --------------  ----- \n 0   Timestamp          40161 non-null  object\n 1   Report Number      40161 non-null  int64 \n 2   City               40161 non-null  object\n 3   Crime Code         40161 non-null  int64 \n 4   Crime Description  40161 non-null  object\n 5   Victim Age         40161 non-null  int64 \n 6   Victim Gender      40161 non-null  object\n 7   Weapon Used        40161 non-null  object\n 8   Crime Domain       40161 non-null  object\n 9   Police Deployed    40161 non-null  int64 \n 10  Case Closed        40161 non-null  object\ndtypes: int64(4), object(7)\nmemory usage: 3.4+ MB\nNone\n\nUsing columns:\nTime column: Timestamp\nCrime type column: Crime Description\nCity column: City\n\nClass distribution:\nCrime Description\nBURGLARY               1980\nVANDALISM              1976\nFRAUD                  1965\nDOMESTIC VIOLENCE      1932\nFIREARM OFFENSE        1931\nROBBERY                1928\nKIDNAPPING             1920\nIDENTITY THEFT         1918\nSEXUAL ASSAULT         1917\nASSAULT                1915\nTRAFFIC VIOLATION      1915\nPUBLIC INTOXICATION    1912\nHOMICIDE               1909\nCYBERCRIME             1899\nILLEGAL POSSESSION     1895\nARSON                  1894\nDRUG OFFENSE           1890\nEXTORTION              1873\nCOUNTERFEITING         1871\nVEHICLE - STOLEN       1862\nSHOPLIFTING            1859\nName: count, dtype: int64\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[9], line 294\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28mprint\u001b[39m(data\u001b[38;5;241m.\u001b[39mhead())\n\u001b[1;32m    293\u001b[0m \u001b[38;5;66;03m# Set up and train the prediction model\u001b[39;00m\n\u001b[0;32m--> 294\u001b[0m model, tokenizer, label_encoder, device \u001b[38;5;241m=\u001b[39m \u001b[43msetup_crime_prediction_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;66;03m# Save the trained model\u001b[39;00m\n\u001b[1;32m    297\u001b[0m model_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcrime_prediction_model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","Cell \u001b[0;32mIn[9], line 209\u001b[0m, in \u001b[0;36msetup_crime_prediction_model\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    207\u001b[0m class_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m/\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(class_counts, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[1;32m    208\u001b[0m class_weights \u001b[38;5;241m=\u001b[39m class_weights \u001b[38;5;241m/\u001b[39m class_weights\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m--> 209\u001b[0m class_weights \u001b[38;5;241m=\u001b[39m class_weights\u001b[38;5;241m.\u001b[39mto(\u001b[43mdevice\u001b[49m)\n\u001b[1;32m    211\u001b[0m \u001b[38;5;66;03m# Add class weights to the model's config\u001b[39;00m\n\u001b[1;32m    212\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mclass_weights \u001b[38;5;241m=\u001b[39m class_weights\u001b[38;5;241m.\u001b[39mtolist()\n","\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'device' referenced before assignment"],"ename":"UnboundLocalError","evalue":"local variable 'device' referenced before assignment","output_type":"error"}],"execution_count":9}]}